# EX_06 - SARSA Learning Algorithm


## AIM

To implement and analyze the SARSA (State-Action-Reward-State-Action) reinforcement learning algorithm for learning optimal policies in a given environment using on-policy temporal difference control.

## PROBLEM STATEMENT

In many real-world and simulated environments, an agent must learn an optimal sequence of actions to maximize cumulative rewards without prior knowledge of the environment's dynamics. The challenge lies in balancing exploration of new actions with exploitation of known rewarding strategies.

This experiment aims to solve the problem of learning an optimal policy for decision-making under uncertainty using the SARSA algorithm. SARSA is an on-policy Temporal Difference (TD) learning algorithm where the agent updates its Q-values based on the action actually taken in the next state. This approach allows the agent to improve its behavior gradually while still exploring the environment.

The goal is to:

- Implement the SARSA algorithm.

- Train the agent in a grid world or similar environment.

- Evaluate how effectively the agent learns an optimal policy over episodes.

- Compare performance metrics such as total reward per episode and convergence behavior.

## SARSA LEARNING ALGORITHM

![image](https://github.com/user-attachments/assets/59a1e43c-abc5-4440-a4fd-79eaf6e13b60)


## SARSA LEARNING FUNCTION
```
DEVELOPED BY : NIRAUNJANA GAYATHRI G R
REGISTER NO. : 212222230096
```
```

```

## OUTPUT:

Mention the optimal policy, optimal value function , success rate for the optimal policy.

Include plot comparing the state value functions of Monte Carlo method and SARSA learning.

## RESULT:

Write your result here
